{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LBP and HOG as Feature Map for feeding into FNNs\n",
    "+ Using Local Binary pattern as feature map to feed to FNN's using Tensorflow\n",
    "+ Using Histogram of Oriented gradients as feature map to feed to FNN's using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unzip\n",
    "import zipfile as zf\n",
    "import os\n",
    "path=os.getcwd()\n",
    "files = zf.ZipFile(\"orl_faces.zip\", 'r')\n",
    "files.extractall(path)\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\n",
      "  Downloading https://files.pythonhosted.org/packages/34/79/cefff573a53ca3fb4c390739d19541b95f371e24d2990aed4cd8837971f0/scikit_image-0.14.0-cp36-cp36m-manylinux1_x86_64.whl (25.3MB)\n",
      "\u001b[K    100% |################################| 25.3MB 28kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow>=4.3.0 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from scikit-image)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from scikit-image)\n",
      "Collecting PyWavelets>=0.4.0 (from scikit-image)\n",
      "  Downloading https://files.pythonhosted.org/packages/32/c0/3646053c0ce297686da524bc968bff6017151a9089d16c33afe7d330a48b/PyWavelets-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (5.7MB)\n",
      "\u001b[K    100% |################################| 5.7MB 124kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from scikit-image)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from scikit-image)\n",
      "Requirement already satisfied: dask[array]>=0.9.0 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from scikit-image)\n",
      "Collecting networkx>=1.8 (from scikit-image)\n",
      "  Downloading https://files.pythonhosted.org/packages/11/42/f951cc6838a4dff6ce57211c4d7f8444809ccbe2134179950301e5c4c83c/networkx-2.1.zip (1.6MB)\n",
      "\u001b[K    100% |################################| 1.6MB 460kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle>=0.2.1 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from scikit-image)\n",
      "Requirement already satisfied: numpy>=1.7.1 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from matplotlib>=2.0.0->scikit-image)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/fastai/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from matplotlib>=2.0.0->scikit-image)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from matplotlib>=2.0.0->scikit-image)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from matplotlib>=2.0.0->scikit-image)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/fastai/lib/python3.6/site-packages (from matplotlib>=2.0.0->scikit-image)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from matplotlib>=2.0.0->scikit-image)\n",
      "Requirement already satisfied: toolz>=0.7.3 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from dask[array]>=0.9.0->scikit-image)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from networkx>=1.8->scikit-image)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/fastai/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->scikit-image)\n",
      "Building wheels for collected packages: networkx\n",
      "  Running setup.py bdist_wheel for networkx ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/44/c0/34/6f98693a554301bdb405f8d65d95bbcd3e50180cbfdd98a94e\n",
      "Successfully built networkx\n",
      "Installing collected packages: PyWavelets, networkx, scikit-image\n",
      "Successfully installed PyWavelets-0.5.2 networkx-2.1 scikit-image-0.14.0\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/fastai/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/fastai/lib/python3.6/site-packages (from sklearn)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Running setup.py bdist_wheel for sklearn ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-image\n",
    "!pip install numpy\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    }
   ],
   "source": [
    "# Load all the images\n",
    "import os\n",
    "import glob\n",
    "import PIL\n",
    "import scipy.misc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "path=os.getcwd()\n",
    "folder=path+\"//orl_faces//\"\n",
    "images=[]\n",
    "width,height=28,28\n",
    "for i in range(1,41):\n",
    "    val=\"s\"+str(i)\n",
    "    final_path=folder+val\n",
    "    temp=[]\n",
    "    for f in Path(final_path).glob('*.pgm'):\n",
    "        img=scipy.misc.imread(f)\n",
    "        #img_resize=scipy.misc.imresize(img, [width,height])\n",
    "        #temp.append(img_resize)\n",
    "        temp.append(img)\n",
    "    images.append(np.asarray(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 (10, 112, 92)\n"
     ]
    }
   ],
   "source": [
    "print(len(images), images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "Image shape of some of the images (112, 92)\n",
      "Image shape of some of the images (112, 92)\n",
      "Image shape of some of the images (112, 92)\n",
      "Image shape of some of the images (112, 92)\n",
      "Image shape of some of the images (112, 92)\n",
      "Image shape of some of the images (112, 92)\n",
      "Image shape of some of the images (112, 92)\n"
     ]
    }
   ],
   "source": [
    "## Veerify once the images are extracted\n",
    "import cv2\n",
    "from PIL import Image\n",
    "print(len(images))\n",
    "for i in range(0,7):\n",
    "    print(\"Image shape of some of the images\", images[0][i].shape)\n",
    "    #newimage=np.reshape(images[0][i],(images[0][i].shape[0], images[0][i].shape[1], 1))\n",
    "    img=Image.fromarray(images[0][i])\n",
    "    name=\"test_image\"+str(i)+\".png\"\n",
    "    img.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 6, 112, 92)\n",
      "(40, 4, 112, 92)\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(Images):\n",
    "    train=[]\n",
    "    test=[]\n",
    "    #print(len(Images[39]))\n",
    "    for i in range(40):\n",
    "        temp_train=[Images[i][j] for j in range(6)]\n",
    "        temp_test=[Images[i][j] for j in range(6,10)]\n",
    "        train.append(np.asarray(temp_train))\n",
    "        test.append(np.asarray(temp_test))\n",
    "    return np.asarray(train), np.asarray(test)\n",
    "\n",
    "X_train, X_test = train_test_split(images)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the labels as well for the train and test\n",
    "def labels():\n",
    "    train_lab, test_lab = [],[]\n",
    "    for i in range(40): #i is the subject so based on that you will one hot encode\n",
    "        temp_=[0]*40\n",
    "        temp_[i]=1\n",
    "        temp_tr,temp_tst=[],[]\n",
    "        for j in range(10):\n",
    "            if j<=5:\n",
    "                temp_tr.append(temp_)\n",
    "            else: \n",
    "                temp_tst.append(temp_)\n",
    "        train_lab.append(temp_tr)\n",
    "        test_lab.append(temp_tst)\n",
    "    return np.asarray(train_lab), np.asarray(test_lab)\n",
    "\n",
    "y_train, y_test = labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (40, 6, 40), test labels shape: (40, 4, 40)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training labels shape: {}, test labels shape: {}\".format(y_train.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the dimensions of the image\n",
      "\n",
      "(512, 512)\n",
      "Check the dimensions of the LBP\n",
      "\n",
      "(512, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotated images matched against references using LBP:\n",
      "original: brick, rotated: 30deg, match result:  brick\n",
      "original: brick, rotated: 70deg, match result:  brick\n",
      "original: grass, rotated: 145deg, match result:  grass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x600 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x600 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "===============================================\n",
    "Local Binary Pattern for texture classification\n",
    "===============================================\n",
    "\n",
    "In this example, we will see how to classify textures based on LBP (Local\n",
    "Binary Pattern). LBP looks at points surrounding a central point and tests\n",
    "whether the surrounding points are greater than or less than the central point\n",
    "(i.e. gives a binary result).\n",
    "\n",
    "Before trying out LBP on an image, it helps to look at a schematic of LBPs.\n",
    "The below code is just used to plot the schematic.\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "METHOD = 'uniform'\n",
    "plt.rcParams['font.size'] = 9\n",
    "\n",
    "\n",
    "def plot_circle(ax, center, radius, color):\n",
    "    circle = plt.Circle(center, radius, facecolor=color, edgecolor='0.5')\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "\n",
    "def plot_lbp_model(ax, binary_values):\n",
    "    \"\"\"Draw the schematic for a local binary pattern.\"\"\"\n",
    "    # Geometry spec\n",
    "    theta = np.deg2rad(45)\n",
    "    R = 1\n",
    "    r = 0.15\n",
    "    w = 1.5\n",
    "    gray = '0.5'\n",
    "\n",
    "    # Draw the central pixel.\n",
    "    plot_circle(ax, (0, 0), radius=r, color=gray)\n",
    "    # Draw the surrounding pixels.\n",
    "    for i, facecolor in enumerate(binary_values):\n",
    "        x = R * np.cos(i * theta)\n",
    "        y = R * np.sin(i * theta)\n",
    "        plot_circle(ax, (x, y), radius=r, color=str(facecolor))\n",
    "\n",
    "    # Draw the pixel grid.\n",
    "    for x in np.linspace(-w, w, 4):\n",
    "        ax.axvline(x, color=gray)\n",
    "        ax.axhline(x, color=gray)\n",
    "\n",
    "    # Tweak the layout.\n",
    "    ax.axis('image')\n",
    "    ax.axis('off')\n",
    "    size = w + 0.2\n",
    "    ax.set_xlim(-size, size)\n",
    "    ax.set_ylim(-size, size)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(ncols=5, figsize=(7, 2))\n",
    "\n",
    "titles = ['flat', 'flat', 'edge', 'corner', 'non-uniform']\n",
    "\n",
    "binary_patterns = [np.zeros(8),\n",
    "                   np.ones(8),\n",
    "                   np.hstack([np.ones(4), np.zeros(4)]),\n",
    "                   np.hstack([np.zeros(3), np.ones(5)]),\n",
    "                   [1, 0, 0, 1, 1, 1, 0, 0]]\n",
    "\n",
    "for ax, values, name in zip(axes, binary_patterns, titles):\n",
    "    plot_lbp_model(ax, values)\n",
    "    ax.set_title(name)\n",
    "\n",
    "######################################################################\n",
    "# The figure above shows example results with black (or white) representing\n",
    "# pixels that are less (or more) intense than the central pixel. When\n",
    "# surrounding pixels are all black or all white, then that image region is\n",
    "# flat (i.e. featureless). Groups of continuous black or white pixels are\n",
    "# considered \"uniform\" patterns that can be interpreted as corners or edges.\n",
    "# If pixels switch back-and-forth between black and white pixels, the pattern\n",
    "# is considered \"non-uniform\".\n",
    "#\n",
    "# When using LBP to detect texture, you measure a collection of LBPs over an\n",
    "# image patch and look at the distribution of these LBPs. Lets apply LBP to a\n",
    "# brick texture.\n",
    "\n",
    "from skimage.transform import rotate\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage import data\n",
    "from skimage.color import label2rgb\n",
    "\n",
    "# settings for LBP\n",
    "radius = 3\n",
    "n_points = 8 * radius\n",
    "\n",
    "\n",
    "def overlay_labels(image, lbp, labels):\n",
    "    mask = np.logical_or.reduce([lbp == each for each in labels])\n",
    "    return label2rgb(mask, image=image, bg_label=0, alpha=0.5)\n",
    "\n",
    "\n",
    "def highlight_bars(bars, indexes):\n",
    "    for i in indexes:\n",
    "        bars[i].set_facecolor('r')\n",
    "\n",
    "\n",
    "image = data.load('brick.png')\n",
    "lbp = local_binary_pattern(image, n_points, radius, METHOD)\n",
    "\n",
    "print(\"Check the dimensions of the image\\n\")\n",
    "print(image.shape)\n",
    "print(\"Check the dimensions of the LBP\\n\")\n",
    "print(lbp.shape)\n",
    "\n",
    "\n",
    "def hist(ax, lbp):\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    return ax.hist(lbp.ravel(), normed=True, bins=n_bins, range=(0, n_bins),\n",
    "                   facecolor='0.5')\n",
    "\n",
    "\n",
    "# plot histograms of LBP of textures\n",
    "fig, (ax_img, ax_hist) = plt.subplots(nrows=2, ncols=3, figsize=(9, 6))\n",
    "plt.gray()\n",
    "\n",
    "titles = ('edge', 'flat', 'corner')\n",
    "w = width = radius - 1\n",
    "edge_labels = range(n_points // 2 - w, n_points // 2 + w + 1)\n",
    "flat_labels = list(range(0, w + 1)) + list(range(n_points - w, n_points + 2))\n",
    "i_14 = n_points // 4            # 1/4th of the histogram\n",
    "i_34 = 3 * (n_points // 4)      # 3/4th of the histogram\n",
    "corner_labels = (list(range(i_14 - w, i_14 + w + 1)) +\n",
    "                 list(range(i_34 - w, i_34 + w + 1)))\n",
    "\n",
    "label_sets = (edge_labels, flat_labels, corner_labels)\n",
    "\n",
    "for ax, labels in zip(ax_img, label_sets):\n",
    "    ax.imshow(overlay_labels(image, lbp, labels))\n",
    "\n",
    "for ax, labels, name in zip(ax_hist, label_sets, titles):\n",
    "    counts, _, bars = hist(ax, lbp)\n",
    "    highlight_bars(bars, labels)\n",
    "    ax.set_ylim(ymax=np.max(counts[:-1]))\n",
    "    ax.set_xlim(xmax=n_points + 2)\n",
    "    ax.set_title(name)\n",
    "\n",
    "ax_hist[0].set_ylabel('Percentage')\n",
    "for ax in ax_img:\n",
    "    ax.axis('off')\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# The above plot highlights flat, edge-like, and corner-like regions of the\n",
    "# image.\n",
    "#\n",
    "# The histogram of the LBP result is a good measure to classify textures.\n",
    "# Here, we test the histogram distributions against each other using the\n",
    "# Kullback-Leibler-Divergence.\n",
    "\n",
    "# settings for LBP\n",
    "radius = 2\n",
    "n_points = 8 * radius\n",
    "\n",
    "\n",
    "def kullback_leibler_divergence(p, q):\n",
    "    p = np.asarray(p)\n",
    "    q = np.asarray(q)\n",
    "    filt = np.logical_and(p != 0, q != 0)\n",
    "    return np.sum(p[filt] * np.log2(p[filt] / q[filt]))\n",
    "\n",
    "\n",
    "def match(refs, img):\n",
    "    best_score = 10\n",
    "    best_name = None\n",
    "    lbp = local_binary_pattern(img, n_points, radius, METHOD)\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, _ = np.histogram(lbp, normed=True, bins=n_bins, range=(0, n_bins))\n",
    "    for name, ref in refs.items():\n",
    "        ref_hist, _ = np.histogram(ref, normed=True, bins=n_bins,\n",
    "                                   range=(0, n_bins))\n",
    "        score = kullback_leibler_divergence(hist, ref_hist)\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_name = name\n",
    "    return best_name\n",
    "\n",
    "\n",
    "brick = data.load('brick.png')\n",
    "grass = data.load('grass.png')\n",
    "wall = data.load('rough-wall.png')\n",
    "\n",
    "refs = {\n",
    "    'brick': local_binary_pattern(brick, n_points, radius, METHOD),\n",
    "    'grass': local_binary_pattern(grass, n_points, radius, METHOD),\n",
    "    'wall': local_binary_pattern(wall, n_points, radius, METHOD)\n",
    "}\n",
    "\n",
    "# classify rotated textures\n",
    "print('Rotated images matched against references using LBP:')\n",
    "print('original: brick, rotated: 30deg, match result: ',\n",
    "      match(refs, rotate(brick, angle=30, resize=False)))\n",
    "print('original: brick, rotated: 70deg, match result: ',\n",
    "      match(refs, rotate(brick, angle=70, resize=False)))\n",
    "print('original: grass, rotated: 145deg, match result: ',\n",
    "      match(refs, rotate(grass, angle=145, resize=False)))\n",
    "\n",
    "# plot histograms of LBP of textures\n",
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(nrows=2, ncols=3,\n",
    "                                                       figsize=(9, 6))\n",
    "plt.gray()\n",
    "\n",
    "ax1.imshow(brick)\n",
    "ax1.axis('off')\n",
    "hist(ax4, refs['brick'])\n",
    "ax4.set_ylabel('Percentage')\n",
    "\n",
    "ax2.imshow(grass)\n",
    "ax2.axis('off')\n",
    "hist(ax5, refs['grass'])\n",
    "ax5.set_xlabel('Uniform LBP values')\n",
    "\n",
    "ax3.imshow(wall)\n",
    "ax3.axis('off')\n",
    "hist(ax6, refs['wall'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Training set with feauture (40, 6, 28, 28), New Test set with feature (40, 4, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "def get_lbp(X_train, X_test):\n",
    "    #do something\n",
    "    # settings for LBP\n",
    "    radius = 2\n",
    "    n_points = 8 * radius\n",
    "    METHOD = 'uniform'\n",
    "    subjects, num_images_train, img_ht, img_wid = X_train.shape\n",
    "    num_images_test=X_test.shape[1]\n",
    "    X_train_new,X_test_new=[],[]\n",
    "    \n",
    "    #Train images for LBP\n",
    "    for i in range(subjects):\n",
    "        temp=[]\n",
    "        for j in range(num_images_train):\n",
    "            img=X_train[i][j]\n",
    "            lbp=local_binary_pattern(img, n_points, radius, METHOD)\n",
    "            temp.append(lbp)\n",
    "        X_train_new.append(np.asarray(temp))\n",
    "    \n",
    "    #Test images for LBP\n",
    "    for i in range(subjects):\n",
    "        temp=[]\n",
    "        for j in range(num_images_test):\n",
    "            img=X_test[i][j]\n",
    "            lbp=local_binary_pattern(img, n_points, radius, METHOD)\n",
    "            temp.append(lbp)\n",
    "        X_test_new.append(np.asarray(temp))\n",
    "    return np.asarray(X_train_new), np.asarray(X_test_new)\n",
    "\n",
    "X_train_transf, X_test_transf = get_lbp(X_train, X_test)\n",
    "print(\"New Training set with feauture {}, New Test set with feature {}\".format(X_train_transf.shape, X_test_transf.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/86/95/274190b39950e1e9eef4b071acefea832ac3e2c19bb4b442fa54f3214d2e/tensorflow-1.9.0-cp36-cp36m-manylinux1_x86_64.whl (51.1MB)\n",
      "\u001b[K    100% |################################| 51.1MB 14kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.10.0,>=1.9.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/1f/3da43860db614e294a034e42d4be5c8f7f0d2c75dc1c428c541116d8cdab/tensorboard-1.9.0-py3-none-any.whl (3.3MB)\n",
      "\u001b[K    100% |################################| 3.3MB 224kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/96/5d/18feb90462c8edaae71305716c7e8bac479fc9dface63221f808a6b95880/absl-py-0.3.0.tar.gz (84kB)\n",
      "\u001b[K    100% |################################| 92kB 7.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/fastai/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting setuptools<=39.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl (566kB)\n",
      "\u001b[K    100% |################################| 573kB 1.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz\n",
      "Collecting protobuf>=3.4.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/fc/f0/db040681187496d10ac50ad167a8fd5f953d115b16a7085e19193a6abfd2/protobuf-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (7.1MB)\n",
      "\u001b[K    100% |################################| 7.1MB 96kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/f6/b1a6a703620ac4c393d286b0289c6bb51294629aa1cae8ef3bc1bcafd164/grpcio-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (9.3MB)\n",
      "\u001b[K    100% |################################| 9.3MB 78kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.10 (from tensorboard<1.10.0,>=1.9.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)\n",
      "\u001b[K    100% |################################| 327kB 2.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard<1.10.0,>=1.9.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl (78kB)\n",
      "\u001b[K    100% |################################| 81kB 6.9MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: absl-py, gast\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/4c/16/ef/e36a23f2432e9220f8845f94e2c3abd39e7d9d1cd458d3159d\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/9a/1f/0e/3cde98113222b853e98fc0a8e9924480a3e25f1b4008cedb4f\n",
      "Successfully built absl-py gast\n",
      "Installing collected packages: werkzeug, markdown, setuptools, protobuf, tensorboard, astor, absl-py, gast, grpcio, tensorflow\n",
      "  Found existing installation: setuptools 39.2.0\n",
      "    Uninstalling setuptools-39.2.0:\n",
      "      Successfully uninstalled setuptools-39.2.0\n",
      "Successfully installed absl-py-0.3.0 astor-0.7.1 gast-0.2.0 grpcio-1.14.0 markdown-2.6.11 protobuf-3.6.0 setuptools-39.1.0 tensorboard-1.9.0 tensorflow-1.9.0 werkzeug-0.14.1\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version 1.9.0\n",
      "Reshaped Train set shape (240, 784), Reshaped Test set shape (160, 784)\n",
      "Reshaped Train labels shape (240, 40), Reshaped Test labels shape (160, 40)\n"
     ]
    }
   ],
   "source": [
    "## Using the Softmax Classifier with Tensorflow\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version \" + tf.__version__)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "# Reshaping the Training set and test set to use with TF\n",
    "reshaped_train_set = np.reshape(X_train_transf, (240, X_train_transf.shape[2], X_train_transf.shape[3]))\n",
    "reshaped_train_set = reshaped_train_set.reshape(-1, 784)\n",
    "reshaped_test_set = np.reshape(X_test_transf, (160, X_test_transf.shape[2], X_test_transf.shape[3]))\n",
    "reshaped_test_set = reshaped_test_set.reshape(-1, 784)\n",
    "print(\"Reshaped Train set shape {}, Reshaped Test set shape {}\".format(reshaped_train_set.shape, reshaped_test_set.shape))\n",
    "\n",
    "# Reshaping the Labels\n",
    "reshaped_train_labels  = np.reshape(y_train, (240, y_train.shape[-1]))\n",
    "reshaped_test_labels   = np.reshape(y_test, (160, y_test.shape[-1]))\n",
    "print(\"Reshaped Train labels shape {}, Reshaped Test labels shape {}\".format(reshaped_train_labels.shape, reshaped_test_labels.shape))\n",
    "\n",
    "# Normalize the feature data before training\n",
    "maximum_tr=max(reshaped_train_set[0])\n",
    "reshaped_train_set_norm = reshaped_train_set/(1.0*maximum_tr)\n",
    "maximum_test=max(reshaped_test_set[0])\n",
    "reshaped_test_set_norm = reshaped_test_set/(1.0*maximum_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version 1.9.0\n"
     ]
    }
   ],
   "source": [
    "'''Trains and evaluates a fully-connected neural net classifier'''\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "## Using the Softmax Classifier with Tensorflow\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version \" + tf.__version__)\n",
    "tf.set_random_seed(0)\n",
    "beginTime = time.time()\n",
    "\n",
    "# Parameter definitions\n",
    "batch_size = 10\n",
    "learning_rate = 0.0003\n",
    "hidden_units=120\n",
    "max_steps = 2000\n",
    "\n",
    "# Define input placeholders for images and labels\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 40])\n",
    "\n",
    "## TODO : Just plain 2 L FNN only\n",
    "\n",
    "'''\n",
    "Wieght Initialization\n",
    "'''\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "'''\n",
    "Convolution and Pooling\n",
    "'''\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size = [filter_size, filter_size, input_channel, number_of_output_channels]\n",
    "W_conv1 = weight_variable([3, 3, 1, 30])\n",
    "b_conv1 = bias_variable([30])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([3, 3, 30, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 40])\n",
    "b_fc2 = bias_variable([40])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     0: training accuracy 0\n",
      "Step   100: training accuracy 0\n",
      "Step   200: training accuracy 0.2\n",
      "Step   300: training accuracy 0.3\n",
      "Step   400: training accuracy 1\n",
      "Step   500: training accuracy 1\n",
      "Step   600: training accuracy 1\n",
      "Step   700: training accuracy 1\n",
      "Step   800: training accuracy 1\n",
      "Step   900: training accuracy 1\n",
      "Test accuracy 0.8375\n",
      "Confusion Matrix [[4 0 0 ... 0 0 0]\n",
      " [0 4 0 ... 0 0 0]\n",
      " [0 0 4 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 4 0 0]\n",
      " [0 0 0 ... 0 4 0]\n",
      " [0 0 0 ... 1 0 1]]\n",
      "Total time: 15.89s\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "beginTime = time.time()\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "confusion_matrix = tf.confusion_matrix(tf.argmax(y_,1), tf.argmax(y_conv,1))\n",
    "\n",
    "max_steps = 1000\n",
    "batch_size=10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Repeat max_steps times\n",
    "    for i in range(max_steps):\n",
    "        # Generate input data batch\n",
    "        indices = np.random.choice(reshaped_train_set_norm.shape[0], batch_size)\n",
    "        images_batch = reshaped_train_set_norm[indices]\n",
    "        labels_batch = reshaped_train_labels[indices]\n",
    "\n",
    "        # Periodically print out the model's current accuracy\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x: images_batch, y_: labels_batch, keep_prob: 1.0})\n",
    "            #train_accuracy = sess.run(accuracy, feed_dict={images_placeholder: images_batch, labels_placeholder: labels_batch})\n",
    "            print('Step {:5d}: training accuracy {:g}'.format(i, train_accuracy))\n",
    "\n",
    "        # Perform a single training step\n",
    "        sess.run(train_step, feed_dict={x: images_batch,y_: labels_batch, keep_prob: 0.5})\n",
    "    # After finishing the training, evaluate on the test set\n",
    "    test_accuracy, conf_mat = sess.run([accuracy, confusion_matrix], feed_dict={x: reshaped_test_set_norm,y_: reshaped_test_labels, keep_prob: 1.0})\n",
    "    print('Test accuracy {:g}'.format(test_accuracy))\n",
    "    print('Confusion Matrix',conf_mat)\n",
    "endTime = time.time()\n",
    "print('Total time: {:5.2f}s'.format(endTime - beginTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong Prediction for class : 17\n",
      "Wrong Prediction for class : 27\n",
      "Wrong Prediction for class : 39\n"
     ]
    }
   ],
   "source": [
    "# Making sense of the confusion matrix\n",
    "for idx,c in enumerate(conf_mat[:]):\n",
    "    if idx!=np.argmax(c, 0):\n",
    "        print(\"Wrong Prediction for class : {}\".format(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for LBP and use of FNN \n",
    "\n",
    "### Input shape to (28x28), Filter-size = 5x5 , test set accuracy ~67%\n",
    "+ One of the Output runs are pretty good as you see below, caught one of the good runs !!\n",
    "```\n",
    "Step     0: training accuracy 0\n",
    "Step   100: training accuracy 0.1\n",
    "Step   200: training accuracy 0.7\n",
    "Step   300: training accuracy 1\n",
    "Step   400: training accuracy 1\n",
    "Step   500: training accuracy 1\n",
    "Step   600: training accuracy 1\n",
    "Step   700: training accuracy 1\n",
    "Step   800: training accuracy 1\n",
    "Step   900: training accuracy 1\n",
    "Test accuracy 0.66875\n",
    "Confusion Matrix [[2 0 0 ... 0 0 0]\n",
    " [0 3 0 ... 0 0 0]\n",
    " [0 0 4 ... 0 0 0]\n",
    " ...\n",
    " [0 0 0 ... 4 0 0]\n",
    " [0 0 0 ... 0 1 0]\n",
    " [0 0 1 ... 0 0 0]]\n",
    "Total time: 77.04s\n",
    "```\n",
    "+ Training accuracy reaches 100%\n",
    "+ Test accuracy reaches almost 67%, which is better than using LBP with Softmax classifier \n",
    "+ Images have been scaled down to fit the NN, remember changing the sizes does affect the performance\n",
    "+ Dataset is extremely small so not much data to work with, data-augmentation will help a lot here, will try that later\n",
    "+ Changing the size of the Inputs can be tried to have better peroformance\n",
    "\n",
    "### Tried changing the Input shape to (92x92), but as you see the test set accuracy is really bad ~ 14%\n",
    "\n",
    "```\n",
    "Step     0: training accuracy 0.1\n",
    "Step   100: training accuracy 0.4\n",
    "Step   200: training accuracy 0.8\n",
    "Step   300: training accuracy 0.8\n",
    "Step   400: training accuracy 1\n",
    "Step   500: training accuracy 1\n",
    "Step   600: training accuracy 1\n",
    "Step   700: training accuracy 1\n",
    "Step   800: training accuracy 1\n",
    "Step   900: training accuracy 1\n",
    "Test accuracy 0.1375\n",
    "Confusion Matrix [[0 0 0 ... 0 0 0]\n",
    " [0 0 0 ... 0 1 0]\n",
    " [0 0 1 ... 0 0 0]\n",
    " ...\n",
    " [0 1 0 ... 0 1 0]\n",
    " [0 0 0 ... 0 0 0]\n",
    " [0 1 0 ... 0 0 0]]\n",
    "Total time: 302.81s\n",
    "```\n",
    "\n",
    "### Final Result : Input shape to (28x28), filter_size = 3x3, accuracy is really good : Test accuracy ~ 84%\n",
    "```\n",
    "Step     0: training accuracy 0\n",
    "Step   100: training accuracy 0\n",
    "Step   200: training accuracy 0.2\n",
    "Step   300: training accuracy 0.3\n",
    "Step   400: training accuracy 1\n",
    "Step   500: training accuracy 1\n",
    "Step   600: training accuracy 1\n",
    "Step   700: training accuracy 1\n",
    "Step   800: training accuracy 1\n",
    "Step   900: training accuracy 1\n",
    "Test accuracy 0.8375\n",
    "Confusion Matrix [[4 0 0 ... 0 0 0]\n",
    " [0 4 0 ... 0 0 0]\n",
    " [0 0 4 ... 0 0 0]\n",
    " ...\n",
    " [0 0 0 ... 4 0 0]\n",
    " [0 0 0 ... 0 4 0]\n",
    " [0 0 0 ... 1 0 1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.4 \n",
    "+ FNN + HOG, using histogram of Gradient as the feature instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/skimage/feature/_hog.py:150: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15. To supress this message specify explicitly the normalization method.\n",
      "  skimage_deprecation)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.feature import hog\n",
    "\n",
    "def get_hog(X_train, X_test):\n",
    "    subjects, num_images_train, img_ht, img_wid = X_train.shape\n",
    "    num_images_test=X_test.shape[1]\n",
    "    X_train_new,X_test_new=[],[]\n",
    "    \n",
    "    #Train images for HOG\n",
    "    for i in range(subjects):\n",
    "        temp=[]\n",
    "        for j in range(num_images_train):\n",
    "            img=X_train[i][j]\n",
    "            hog_=hog(img, orientations=8, pixels_per_cell=(16, 16),cells_per_block=(1, 1))\n",
    "            temp.append(hog_)\n",
    "        X_train_new.append(np.asarray(temp))\n",
    "    \n",
    "    #Test images for Hog\n",
    "    for i in range(subjects):\n",
    "        temp=[]\n",
    "        for j in range(num_images_test):\n",
    "            img=X_test[i][j]\n",
    "            hog_=hog(img, orientations=8, pixels_per_cell=(16, 16),cells_per_block=(1, 1))\n",
    "            temp.append(hog_)\n",
    "        X_test_new.append(np.asarray(temp))\n",
    "    return np.asarray(X_train_new), np.asarray(X_test_new)\n",
    "\n",
    "X_train_hog_, X_test_hog_ = get_hog(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Traing set and Test set after HOG transformations (40, 6, 280), (40, 4, 280)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the Traing set and Test set after HOG transformations {}, {}\".format(X_train_hog_.shape, X_test_hog_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version 1.9.0\n",
      "Reshaped Train set shape (240, 280), Reshaped Test set shape (160, 280)\n"
     ]
    }
   ],
   "source": [
    "## Using the Softmax Classifier with Tensorflow\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version \" + tf.__version__)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "# Reshaping the Training set and test set to use with TF\n",
    "reshaped_train_set_hog = np.reshape(X_train_hog_, (240, X_train_hog_.shape[2]))\n",
    "reshaped_train_set_hog = reshaped_train_set_hog.reshape(-1, 280)\n",
    "reshaped_test_set_hog = np.reshape(X_test_hog_, (160, X_test_hog_.shape[2]))\n",
    "reshaped_test_set_hog = reshaped_test_set_hog.reshape(-1, 280)\n",
    "print(\"Reshaped Train set shape {}, Reshaped Test set shape {}\".format(reshaped_train_set_hog.shape, reshaped_test_set_hog.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Train set shape (240, 256), Reshaped Test set shape (160, 256)\n"
     ]
    }
   ],
   "source": [
    "# reshape to use in Tensor flow graph\n",
    "reshapeto=256\n",
    "reshaped_train_set_hog_fin,reshaped_test_set_hog_fin=[],[]\n",
    "\n",
    "for idx in range(240):\n",
    "    reshaped_train_set_hog_fin.append(reshaped_train_set_hog[idx][:256])\n",
    "\n",
    "for idx in range(160):\n",
    "    reshaped_test_set_hog_fin.append(reshaped_test_set_hog[idx][:256])\n",
    "\n",
    "\n",
    "reshaped_train_set_hog_fin=np.asarray(reshaped_train_set_hog_fin)\n",
    "reshaped_test_set_hog_fin=np.asarray(reshaped_test_set_hog_fin)\n",
    "\n",
    "\n",
    "print(\"Reshaped Train set shape {}, Reshaped Test set shape {}\".format(reshaped_train_set_hog_fin.shape, reshaped_test_set_hog_fin.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version 1.9.0\n"
     ]
    }
   ],
   "source": [
    "'''Trains and evaluates a fully-connected neural net classifier'''\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "## Using the Softmax Classifier with Tensorflow\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version \" + tf.__version__)\n",
    "tf.set_random_seed(0)\n",
    "beginTime = time.time()\n",
    "\n",
    "# Parameter definitions\n",
    "batch_size = 10\n",
    "learning_rate = 0.0003\n",
    "hidden_units=120\n",
    "max_steps = 2000\n",
    "\n",
    "# Define input placeholders for images and labels\n",
    "x = tf.placeholder(tf.float32, shape=[None, 256])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 40])\n",
    "\n",
    "## TODO : Just plain 2 L FNN only\n",
    "\n",
    "'''\n",
    "Wieght Initialization\n",
    "'''\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "'''\n",
    "Convolution and Pooling\n",
    "'''\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size = [filter_size, filter_size, input_channel, number_of_output_channels]\n",
    "W_conv1 = weight_variable([5, 5, 1, 16])\n",
    "b_conv1 = bias_variable([16])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 16, 16, 1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 16, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([4 * 4 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 4*4*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 40])\n",
    "b_fc2 = bias_variable([40])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Train labels shape (240, 40), Reshaped Test labels shape (160, 40)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the Labels\n",
    "reshaped_train_labels  = np.reshape(y_train, (240, y_train.shape[-1]))\n",
    "reshaped_test_labels   = np.reshape(y_test, (160, y_test.shape[-1]))\n",
    "print(\"Reshaped Train labels shape {}, Reshaped Test labels shape {}\".format(reshaped_train_labels.shape, reshaped_test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     0: training accuracy 0\n",
      "Step   100: training accuracy 0.1\n",
      "Step   200: training accuracy 0.1\n",
      "Step   300: training accuracy 0.1\n",
      "Step   400: training accuracy 0.4\n",
      "Step   500: training accuracy 0.2\n",
      "Step   600: training accuracy 0.7\n",
      "Step   700: training accuracy 0.9\n",
      "Step   800: training accuracy 0.9\n",
      "Step   900: training accuracy 1\n",
      "Step  1000: training accuracy 1\n",
      "Step  1100: training accuracy 1\n",
      "Step  1200: training accuracy 1\n",
      "Step  1300: training accuracy 1\n",
      "Step  1400: training accuracy 1\n",
      "Step  1500: training accuracy 1\n",
      "Step  1600: training accuracy 1\n",
      "Step  1700: training accuracy 1\n",
      "Step  1800: training accuracy 1\n",
      "Step  1900: training accuracy 1\n",
      "Test accuracy 0.925\n",
      "Confusion Matrix [[3 0 0 ... 0 0 0]\n",
      " [0 4 0 ... 0 0 0]\n",
      " [0 0 4 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 4 0 0]\n",
      " [0 0 0 ... 0 4 0]\n",
      " [0 0 0 ... 0 0 3]]\n",
      "Total time: 18.00s\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "beginTime = time.time()\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "confusion_matrix = tf.confusion_matrix(tf.argmax(y_,1), tf.argmax(y_conv,1))\n",
    "\n",
    "max_steps = 2000\n",
    "batch_size=10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Repeat max_steps times\n",
    "    for i in range(max_steps):\n",
    "        # Generate input data batch\n",
    "        indices = np.random.choice(reshaped_train_set_hog_fin.shape[0], batch_size)\n",
    "        images_batch = reshaped_train_set_hog_fin[indices]\n",
    "        labels_batch = reshaped_train_labels[indices]\n",
    "\n",
    "        # Periodically print out the model's current accuracy\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x: images_batch, y_: labels_batch, keep_prob: 1.0})\n",
    "            #train_accuracy = sess.run(accuracy, feed_dict={images_placeholder: images_batch, labels_placeholder: labels_batch})\n",
    "            print('Step {:5d}: training accuracy {:g}'.format(i, train_accuracy))\n",
    "\n",
    "        # Perform a single training step\n",
    "        sess.run(train_step, feed_dict={x: images_batch,y_: labels_batch, keep_prob: 0.5})\n",
    "    # After finishing the training, evaluate on the test set\n",
    "    test_accuracy, conf_mat = sess.run([accuracy, confusion_matrix], feed_dict={x: reshaped_test_set_hog_fin,y_: reshaped_test_labels, keep_prob: 1.0})\n",
    "    print('Test accuracy {:g}'.format(test_accuracy))\n",
    "    print('Confusion Matrix',conf_mat)\n",
    "endTime = time.time()\n",
    "print('Total time: {:5.2f}s'.format(endTime - beginTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong Prediction for class : 27\n"
     ]
    }
   ],
   "source": [
    "# Making sense of the confusion matrix\n",
    "for idx,c in enumerate(conf_mat[:]):\n",
    "    if idx!=np.argmax(c, 0):\n",
    "        print(\"Wrong Prediction for class : {}\".format(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for HOG and use of FNN \n",
    "\n",
    "### Input shape to (16x16), Filter-size = 5x5 , number of epochs=2000, test set accuracy ~93%\n",
    "+ This is the best possible run, where I get almost 100% accuracy\n",
    "```\n",
    "Step     0: training accuracy 0\n",
    "Step   100: training accuracy 0.1\n",
    "Step   200: training accuracy 0.1\n",
    "Step   300: training accuracy 0.1\n",
    "Step   400: training accuracy 0.4\n",
    "Step   500: training accuracy 0.2\n",
    "Step   600: training accuracy 0.7\n",
    "Step   700: training accuracy 0.9\n",
    "Step   800: training accuracy 0.9\n",
    "Step   900: training accuracy 1\n",
    "Step  1000: training accuracy 1\n",
    "Step  1100: training accuracy 1\n",
    "Step  1200: training accuracy 1\n",
    "Step  1300: training accuracy 1\n",
    "Step  1400: training accuracy 1\n",
    "Step  1500: training accuracy 1\n",
    "Step  1600: training accuracy 1\n",
    "Step  1700: training accuracy 1\n",
    "Step  1800: training accuracy 1\n",
    "Step  1900: training accuracy 1\n",
    "Test accuracy 0.925\n",
    "Confusion Matrix [[3 0 0 ... 0 0 0]\n",
    " [0 4 0 ... 0 0 0]\n",
    " [0 0 4 ... 0 0 0]\n",
    " ...\n",
    " [0 0 0 ... 4 0 0]\n",
    " [0 0 0 ... 0 4 0]\n",
    " [0 0 0 ... 0 0 3]]\n",
    "Total time: 18.00s\n",
    "```\n",
    "+ Training accuracy reaches 100%\n",
    "+ Test accuracy reaches almost 93%, which is better than anyother method used here \n",
    "+ My HOG output length 280, so I reduced and used only 256 out of 280 for each image, that reduces some of the information, but that was \n",
    "necessary to using 16x16 as my input to the FNN\n",
    "+ Dataset is extremely small so not much data to work with, data-augmentation will help a lot here, will try that later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
