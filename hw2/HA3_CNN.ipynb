{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import sklearn.metrics as metrics\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asiaynrf\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowpy36\\lib\\site-packages\\skimage\\feature\\_hog.py:150: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15. To supress this message specify explicitly the normalization method.\n",
      "  skimage_deprecation)\n",
      "C:\\Users\\asiaynrf\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflowpy36\\lib\\site-packages\\skimage\\feature\\_hog.py:248: skimage_deprecation: Argument `visualise` is deprecated and will be changed to `visualize` in v0.16\n",
      "  'be changed to `visualize` in v0.16', skimage_deprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10262, 1764)\n",
      "(1292, 1764)\n"
     ]
    }
   ],
   "source": [
    "from fea_util import *\n",
    "\n",
    "#################################################################################\n",
    "###################### step 0:      loading data   ############################\n",
    "#################################################################################\n",
    "train_data, train_label, test_data, test_label = get_data()\n",
    "\n",
    "#################################################################################\n",
    "###################### step 1:      extract feature histograms   ################\n",
    "#################################################################################\n",
    "\n",
    "# set parameters for HOG \n",
    "\n",
    "orient = 9\n",
    "pix_per_cell = 8\n",
    "cell_per_block = 2\n",
    "\n",
    "train_hog_feature, test_hog_feature = get_hog_feature(train_data, test_data, orient, pix_per_cell, cell_per_block)\n",
    "train_hog_feature = np.array(train_hog_feature)\n",
    "test_hog_feature = np.array(test_hog_feature)\n",
    "train_hog_feature = train_hog_feature.reshape(len(train_label), -1)\n",
    "test_hog_feature = test_hog_feature.reshape(len(test_hog_feature), -1)\n",
    "\n",
    "print(train_hog_feature.shape)\n",
    "print(test_hog_feature.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the original data\n",
    "The original a batch data is (10262 x 64 x 64) dimensional tensor expressed in numpy array, where the number of columns, (10262), indicates the number of sample data. As stated in the dataset, the row vector, (1764) represents an hog of the image of 64x64 pixels. Since this project is going to use CNN for the classification tasks, the row vector, (1764), is not an appropriate form of image data to feed. In order to feed an image data into a CNN model, the dimension of the tensor representing an image data should be either (width x height x num_channel) or (num_channel x width x height). It depends on your choice (check out the tensorflow conv2d). In this particular project, I am going to use the dimension of the first choice because the default choice in tensorflow's CNN operation is so.\n",
    "\n",
    "num_channel=1 for this case\n",
    "\n",
    "[O] **need to be modified into a new shape**\n",
    "\n",
    "#### Understanding the original labels\n",
    "The label data is just a list of 10262 numbers in the range 1-68, which corresponds to each of the 68 classes\n",
    "\n",
    "[X] **need to be modified into a new shape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_data_changed(train_x_feature, test_x_feature):\n",
    "    train_features = train_x_feature.reshape((len(train_x_feature), 1, 42, 42)).transpose(0,2,3,1)\n",
    "    test_features = test_x_feature.reshape((len(test_x_feature), 1, 42, 42)).transpose(0,2,3,1)\n",
    "    return train_features, test_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_stats(train_feature, test_feature):\n",
    "    train_features,test_features= load_data_changed(train_feature, test_feature)\n",
    "    return train_features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature, test_feauture = display_stats(train_hog_feature, test_hog_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape = (10262, 42, 42, 1), Test set shape = (1292, 42, 42, 1)\n",
      "Train labels shape = 10262, Test labels shape = 1292\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of all of the above \n",
    "print(\"Train set shape = {}, Test set shape = {}\".format(train_feature.shape, test_feauture.shape))\n",
    "print(\"Train labels shape = {}, Test labels shape = {}\".format(len(train_label), len(test_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    x: input feature data or image data in numpy array [42, 42, 1]\n",
    "    output: normalized x\n",
    "    \"\"\"\n",
    "    min_val=np.min(x)\n",
    "    max_val=np.max(x)\n",
    "    x=(x-min_val)/(max_val-min_val)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encode of the Labels\n",
    "one_hot_encode function takes the input, x, which is a list of labels(ground truth). The total number of element in the list is the total number of samples in a batch. one_hot_encode function returns a 2 dimensional tensor, where the number of row is the size of the batch, and the number of column is the number of image classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode\n",
    "def one_hot_encoding(x):\n",
    "    \"\"\"\n",
    "    x: A list of labels\n",
    "    return one hot encoded matrix [10262, 68]\n",
    "    \"\"\"\n",
    "    encoded = np.zeros((len(x), 68))\n",
    "    for idx, value in enumerate(x):\n",
    "        encoded[idx][value-1]=1\n",
    "        \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing and one hot encoding the data\n",
    "train_label = one_hot_encoding(train_label)\n",
    "test_label = one_hot_encoding(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10262, 68)\n"
     ]
    }
   ],
   "source": [
    "print(train_label.shape) # now the labels are one hot encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the tensorflow part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = tf.placeholder(tf.float32, shape=(None, 42, 42, 1), name='input_x')\n",
    "y =  tf.placeholder(tf.float32, shape=(None, 68), name='output_y')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convnet(x, keep_prob):\n",
    "    conv1_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 1, 32],   mean=0, stddev=0.08))\n",
    "    conv2_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 32, 64],  mean=0, stddev=0.08))\n",
    "    conv3_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
    "    \n",
    "    # 1, 2\n",
    "    conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv1_bn = tf.layers.batch_normalization(conv1_pool)\n",
    "    \n",
    "    # 3, 4\n",
    "    conv2 = tf.nn.conv2d(conv1_bn, conv2_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')    \n",
    "    conv2_bn = tf.layers.batch_normalization(conv2_pool)\n",
    "    \n",
    "    # 5, 6\n",
    "    conv3 = tf.nn.conv2d(conv2_bn, conv3_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "    conv3_pool = tf.nn.max_pool(conv3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')  \n",
    "    conv3_bn = tf.layers.batch_normalization(conv3_pool)\n",
    "    \n",
    "    # 7\n",
    "    flat = tf.contrib.layers.flatten(conv3_bn)\n",
    "    \n",
    "    # 8\n",
    "    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)\n",
    "    full1 = tf.nn.dropout(full1, keep_prob)\n",
    "    full1 = tf.layers.batch_normalization(full1)\n",
    "    \n",
    "     # 9\n",
    "    out = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=68, activation_fn=None)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs  =  10\n",
    "batch_sizebatch_s  = 128\n",
    "keep_probability = 0.7\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-dcd06371caf8>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-15-dcd06371caf8>:9: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n"
     ]
    }
   ],
   "source": [
    "logits=convnet(x, keep_prob)\n",
    "model=tf.identity(logits, 'logits')\n",
    "\n",
    "#loss and optimizer\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#Accuracy\n",
    "correct_pred=tf.equal(tf.argmax(logits, 1), tf.arg_max(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    session.run(optimizer,\n",
    "                feed_dict={\n",
    "                    x:feature_batch,\n",
    "                    y:label_batch,\n",
    "                    keep_prob:keep_probability\n",
    "                }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    loss=sess.run(cost,\n",
    "                  feed_dict={\n",
    "                      x: feature_batch,\n",
    "                      y:label_batch,\n",
    "                      keep_prob:1\n",
    "                  })\n",
    "    valid_acc=sess.run(accuracy,\n",
    "                      feed_dict={\n",
    "                          x:feature_batch,\n",
    "                          y:label_batch,\n",
    "                          keep_prob:1\n",
    "                      })\n",
    "    print('Loss: {:>10.4f}, Training set accuracy: {:.6f}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch(data, batch_size, num_iter):\n",
    "    data =  np.array(data)\n",
    "    index = len(data)\n",
    "    for i in range(num_iter):\n",
    "        index += batch_size\n",
    "        if (index + batch_size > len(data)):\n",
    "            index = 0\n",
    "            shuffled_indices = np.random.permutation(np.arange(len(data)))\n",
    "            data = data[shuffled_indices]\n",
    "        yield data[index:index + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "train_dir='tf_logs'\n",
    "logdir = train_dir + '/' + datetime.now().strftime('%Y%m%d-%H%M%S') + '/'\n",
    "\n",
    "# Operation merging summary data for TensorBoard\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "# Define saver to save model state at checkpoints\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... Training the model\n",
      "Loss:     4.2004, Training set accuracy: 0.046875\n",
      "Loss:     2.7770, Training set accuracy: 0.476562\n",
      "Loss:     0.6083, Training set accuracy: 0.882812\n",
      "Loss:     0.3488, Training set accuracy: 0.929688\n",
      "Loss:     0.0566, Training set accuracy: 0.984375\n",
      "Loss:     0.0257, Training set accuracy: 1.000000\n",
      "Loss:     0.0116, Training set accuracy: 1.000000\n",
      "Loss:     0.0041, Training set accuracy: 1.000000\n",
      "Loss:     0.0020, Training set accuracy: 1.000000\n",
      "Loss:     0.0026, Training set accuracy: 1.000000\n",
      "Saved checkpoint\n",
      "Loss:     0.0017, Training set accuracy: 1.000000\n",
      "Loss:     0.0005, Training set accuracy: 1.000000\n",
      "Loss:     0.0005, Training set accuracy: 1.000000\n",
      "Loss:     0.0009, Training set accuracy: 1.000000\n",
      "Loss:     0.0002, Training set accuracy: 1.000000\n",
      "Loss:     0.0003, Training set accuracy: 1.000000\n",
      "Loss:     0.0004, Training set accuracy: 1.000000\n",
      "Loss:     0.0009, Training set accuracy: 1.000000\n",
      "Loss:     0.0001, Training set accuracy: 1.000000\n",
      "Loss:     0.0003, Training set accuracy: 1.000000\n",
      "Saved checkpoint\n",
      "Test accuracy 0.97678\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "import os\n",
    "max_steps=2000\n",
    "save_model_pathsave_mod  = './image_classification'\n",
    "print(\".... Training the model\")\n",
    "with tf.Session() as sess:\n",
    "    #Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "    #Generate input data batches\n",
    "    zipped_data = zip(train_feature, train_label)\n",
    "    batches=gen_batch(list(zipped_data), batch_sizebatch_s,max_steps)\n",
    "    \n",
    "    for i in range(max_steps):\n",
    "        batch = next(batches)\n",
    "        batch_features, batch_labels = zip(*batch)\n",
    "        \n",
    "        train_nn(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        \n",
    "        # Periodically print out the model's current accuracy\n",
    "        if i % 100 == 0: show_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "        \n",
    "        # Periodically save checkpoint\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            checkpoint_file = os.path.join(train_dir, 'checkpoint')\n",
    "            saver.save(sess, checkpoint_file, global_step=i)\n",
    "            print('Saved checkpoint')\n",
    "     # After finishing the training, evaluate on the test set\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={\n",
    "    x: test_feauture,\n",
    "    y: test_label,\n",
    "    keep_prob:1})\n",
    "    print('Test accuracy {:g}'.format(test_accuracy))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The accuracy is found to be 97% which is very high as this model performs the best of all the Models present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
